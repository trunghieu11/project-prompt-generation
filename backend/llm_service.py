import os
import google.generativeai as genai
from openai import OpenAI
import json
from typing import List, Dict, Any

# Configure Gemini or OpenAI
# Ensure GEMINI_API_KEY or OPENAI_API_KEY is set in environment variables

def get_api_key():
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        api_key = os.getenv("OPENAI_API_KEY")
    return api_key

async def generate_next_question(idea: str, history: List[Any], current_phase: str) -> Dict[str, Any]:
    # ... (API Key setup same as before)
    gemini_key = os.getenv("GEMINI_API_KEY")
    openai_key = os.getenv("OPENAI_API_KEY")
    
    if not gemini_key and not openai_key:
         # Mock response ...
         pass

    # Construct Prompt Context
    history_text = "\n".join([f"Q: {h.question}\nA: {h.selected_option}" for h in history])
    
    system_instruction = f"""
    You are an expert product manager and software architect helping a user define a software project.
    
    CURRENT PHASE: **{current_phase}**
    
    Task: Generate the NEXT most important clarifying question to better understand requirements, specifically related to the **{current_phase}** of the project.
    
    CRITICAL INSTRUCTIONS:
    1. **Focus Strictly on {current_phase}**: Do NOT ask about other topics.
       - If phase is 'Core Features', ask about functionality.
       - If phase is 'Tech Stack', ask about languages/frameworks.
       - If phase is 'UI/UX', ask about design.
       - If phase is 'Data Strategy', ask about databases/schema.
       - If phase is 'Security', ask about auth/compliance.
       - If phase is 'Testing Strategy', ask about unit/integration/E2E testing or TDD.
       - If phase is 'DevOps', ask about hosting/CI-CD.
       - If phase is 'Observability', ask about logging/monitoring/alerting.
    2. **Formulate a Question**: Ask a specific question related to {current_phase}.
    3. **Generate Options**: Provide 5 distinct, realistic, and relevant answers to YOUR question.
       - WARNING: Do NOT use generic terms (like "Frontend", "Backend") as options unless they are actual answers.
    
    4. **Context Aware**: If the user said it's a "Simple UI", ask about color schemes or layout, don't ask if they need a UI.
    5. **Avoid Duplicates**: Do NOT ask about something already covered in the "Previous Q&A".
    
    Format: Return a VALID JSON object with the following structure:
    {{
        "text": "The question text here",
        "options": ["Option 1", "Option 2", "Option 3", "Option 4", "Option 5"]
    }}
    Make sure the options are distinct. Should be strictly JSON.
    """
    
    user_prompt = f"""
    Project Idea: {idea}
    
    Previous Q&A:
    {history_text}
    """

    try:
        result = {}
        if gemini_key:
            genai.configure(api_key=gemini_key)
            model = genai.GenerativeModel('gemini-1.5-flash')
            full_prompt = f"{system_instruction}\n\n{user_prompt}"
            response = model.generate_content(full_prompt, generation_config={"response_mime_type": "application/json"})
            result = json.loads(response.text)
        elif openai_key:
            client = OpenAI(api_key=openai_key)
            completion = client.chat.completions.create(
                model="gpt-3.5-turbo-0125",
                messages=[
                    {"role": "system", "content": system_instruction},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"}
            )
            result = json.loads(completion.choices[0].message.content)

        # Ensure we have "Let Agent Decide" and "Other"
        options = result.get("options", [])
        
        # Remove if accidentally generated by LLM to avoid dupes
        if "Let Agent Decide (Recommended)" in options:
            options.remove("Let Agent Decide (Recommended)")
        if "Other" in options:
             options.remove("Other")

        # Standardize strictly to 5 LLM options + Agent + Other
        options = options[:5] 
        options.append("Let Agent Decide (Recommended)")
        options.append("Other")
        
        result["options"] = options
        return result

    except Exception as e:
        print(f"Error generating question: {e}")
        return {
            "text": f"Error: {str(e)}",
            "options": ["Retry", "Check Logs", "Option 3", "Option 4", "Let Agent Decide (Recommended)", "Other"]
        }

async def generate_explanation(idea: str, question: str, options: List[str]) -> Dict[str, Any]:
    gemini_key = os.getenv("GEMINI_API_KEY")
    openai_key = os.getenv("OPENAI_API_KEY")

    prompt = f"""
    You are an expert product manager. The user is defining a project: "{idea}".
    
    We asked them: "{question}"
    
    The options are:
    {json.dumps(options)}
    
    Task:
    1. Explain WHY asking this question is important for the project (technical/UX/product context).
    2. Briefly explain the implication of EACH option.
    
    Format: JSON dictionary
    {{
        "question_explanation": "Why this question matters...",
        "option_explanations": {{
            "Option 1": "Explanation...",
            "Option 2": "Explanation...",
            ...
        }}
    }}
    """
    
    try:
        if gemini_key:
            genai.configure(api_key=gemini_key)
            model = genai.GenerativeModel('gemini-1.5-flash')
            response = model.generate_content(prompt, generation_config={"response_mime_type": "application/json"})
            return json.loads(response.text)
        elif openai_key:
            client = OpenAI(api_key=openai_key)
            completion = client.chat.completions.create(
                model="gpt-3.5-turbo-0125",
                messages=[{"role": "system", "content": "You are a helpful assistant. Output strictly JSON."}, {"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )
            return json.loads(completion.choices[0].message.content)
    except Exception as e:
        return {
            "question_explanation": "Could not generate explanation.",
            "option_explanations": {}
        }

async def generate_final_prompt(idea: str, answers: List[Any], selected_phases: List[str]) -> str:
    gemini_key = os.getenv("GEMINI_API_KEY")
    openai_key = os.getenv("OPENAI_API_KEY")
    
    if not gemini_key and not openai_key:
         return f"Mock Final Prompt for '{idea}'. (No API Key provided). User answers count: {len(answers)}"

    history_text = "\n".join([f"Q: {h.question}\nA: {h.selected_option}" for h in answers])
    
    # Calculate missing phases
    all_phases = ['Core Features', 'Tech Stack', 'UI/UX Design', 'Data Strategy', 'Security & Privacy', 'Testing Strategy', 'DevOps & Scalability', 'Observability & Maintenance']
    missing_phases = [p for p in all_phases if p not in selected_phases]
    missing_phases_text = ", ".join(missing_phases)

    system_instruction = f"""
    You are an expert software architect and prompt engineer.
    Task: Write a comprehensive, detailed, and structured prompt that can be fed into an AI coding agent (like yourself) to build this project.
    
    CRITICAL INSTRUCTION FOR "Let Agent Decide":
    - If the user selected "Let Agent Decide (Recommended)" for any question, YOU must make the best technical decision for that requirement based on the project context and industry best practices.
    - Explicitly state in the final prompt: "For [Requirement], the recommended approach is [Decision] because [Reason]."
    
    CRITICAL INSTRUCTION FOR UNSELECTED CATEGORIES:
    - The user explicitly skipped questions for the following categories: [{missing_phases_text}].
    - For these categories, YOU MUST AUTO-GENERATE the best possible recommendations based on the project idea.
    - Treat these exactly like "Let Agent Decide" - provide detailed, expert recommendations for them in the final prompt.
    
    The prompt should include:
    1. Project Overview
    2. Core Features
    3. Tech Stack
    4. UI/UX Design
    5. Technical Constraints
    6. Step-by-Step Implementation Plan
    
    Write the response in Markdown format.
    """

    user_prompt = f"""
    Project Idea: {idea}
    User Requirements (collected via Q&A):
    {history_text}
    """

    try:
        if gemini_key:
            genai.configure(api_key=gemini_key)
            model = genai.GenerativeModel('gemini-1.5-flash')
            full_prompt = f"{system_instruction}\n\n{user_prompt}"
            response = model.generate_content(full_prompt)
            return response.text
        elif openai_key:
            client = OpenAI(api_key=openai_key)
            completion = client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=[
                    {"role": "system", "content": system_instruction},
                    {"role": "user", "content": user_prompt}
                ]
            )
            return completion.choices[0].message.content
            
    except Exception as e:
        return f"Error generating prompt: {e}"
